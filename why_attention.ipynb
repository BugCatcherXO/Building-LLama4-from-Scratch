{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46ed152",
   "metadata": {},
   "source": [
    "# Why Do We Use Attention Instead of Static Embeddings?\n",
    "\n",
    "Large Language Models (LLMs) like GPT or LLaMA are not just memorizing sequences of words.  \n",
    "They rely on **contextual representations**, built dynamically with the **Self-Attention mechanism**.  \n",
    "\n",
    "This section explains **why attention is needed** and what would happen if we only used static embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The limitation of static embeddings\n",
    "\n",
    "- A **static embedding** gives each word a fixed vector, no matter where it appears.  \n",
    "- That means the word **\"ran\"** always has the same vector, whether in:\n",
    "  - *\"The dog ran quickly\"*  \n",
    "  - *\"The program ran successfully\"*  \n",
    "\n",
    "Problem: with static embeddings, the model cannot adapt the meaning of \"ran\" to the surrounding words.  \n",
    "It would try to predict the next word using only this fixed vector, losing critical context.  \n",
    "\n",
    "### Example (without attention):\n",
    "- Input: *\"The dog ran\"*  \n",
    "  - Static embedding of \"ran\" = `[0.5, 0.1, 0.3]` (same everywhere).  \n",
    "  - The model tries to predict the next word based only on that fixed vector.  \n",
    "  - Result: It might memorize that \"quickly\" often follows \"ran\", but cannot generalize well.  \n",
    "\n",
    "- Input: *\"The program ran\"*  \n",
    "  - Same embedding for \"ran\".  \n",
    "  - The model cannot know that here \"successfully\" makes more sense than \"quickly\".  \n",
    "\n",
    "**Conclusion:** Static embeddings = same meaning everywhere → poor generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What attention changes\n",
    "\n",
    "With **Self-Attention**, every token builds a **contextual semantic vector**.  \n",
    "This means:  \n",
    "- \"ran\" does not have one fixed meaning.  \n",
    "- Instead, its **output vector depends on the surrounding words**.  \n",
    "\n",
    "### Example (with attention):\n",
    "- Input: *\"The dog ran quickly\"*  \n",
    "  - Query(Key, Value) interactions allow \"ran\" to pay attention to \"dog\".  \n",
    "  - The output vector of \"ran\" captures \"this is a physical action performed by an animal\".  \n",
    "  - Prediction: \"quickly\" becomes very likely.  \n",
    "\n",
    "- Input: *\"The program ran successfully\"*  \n",
    "  - \"ran\" attends strongly to \"program\".  \n",
    "  - The output vector of \"ran\" changes: \"this is a software action\".  \n",
    "  - Prediction: \"successfully\" becomes very likely.  \n",
    "\n",
    "Attention = **a unique semantic vector per word per context**.  \n",
    "This is the real power of LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why not just big lookup tables?\n",
    "\n",
    "One might ask: *“But during training, don’t we already learn probabilities of next words? Why all this attention machinery?”*  \n",
    "\n",
    "The answer:  \n",
    "- If we only memorized transitions (like \"ran → quickly\"), the model would need to see **every possible sentence** during training.  \n",
    "- With attention, the model can **compose knowledge dynamically**:\n",
    "  - It never needs to see *\"The cat ran swiftly\"* during training.  \n",
    "  - If it knows how *\"cat\"* interacts with verbs, and how *\"swiftly\"* works as an adverb, it can generate it naturally.  \n",
    "\n",
    "Attention = **generalization to unseen contexts**.  \n",
    "It’s not just memorization, but building meaning from relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "- **Static embeddings** = one meaning per word, everywhere → no context sensitivity.  \n",
    "- **Attention-based embeddings** = one meaning per word **per context** → rich and adaptive.  \n",
    "- This makes LLMs capable of:  \n",
    "  - Understanding nuanced differences.  \n",
    "  - Predicting words in sentences never seen before.  \n",
    "  - Generating fluent, context-aware text.  \n",
    "\n",
    "Without attention, LLMs would be nothing more than giant memorization machines.  \n",
    "With attention, they become **contextual reasoning systems**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfa8be",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
